{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MICRO-452 Basics of mobile robotics\n",
    "\n",
    "## Project Report\n",
    "\n",
    "Team  13:\n",
    "\n",
    "Wilhelm Laetitia,\n",
    "Beuret Sylvain,\n",
    "Labbe Victor,\n",
    "Jaffal Oussama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [Global Set-up](#globalsetup)\n",
    "    * [Map Set-up](#mapsetup)\n",
    "    * [Libraries & constants](#libraries)\n",
    "    * [Launch demo](#demo)\n",
    "    * [Main.py structure](#main) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [Computer vision](#computervision)\n",
    "    * [Map detection](#mapdetection)\n",
    "    * [ArUcO detection](#arucodetection)\n",
    "    * [Obstacles detection](#obstaclesdetection)\n",
    "    * [Results](#CVresults)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [Path planning](#pathplanning)\n",
    "    * [Global Navigation](#globalnavigation)\n",
    "    * [A* algorithm](#Aalgorithm)\n",
    "    * [Implementation](#implementation)\n",
    "    * [Results](#PPresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [Kalmann Filtering](#filtering)\n",
    "    * [General, filtering](#genfiltering)\n",
    "    * [Mathematical model](#mathmodel)\n",
    "    * [Filter blabla](#filterblabla)\n",
    "    * [Results](#Kresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. [Motion control](#motioncontrol)\n",
    "    * [Orientation & angle error](#orientation)\n",
    "    * [PD controler](#controler)\n",
    "    * [Compute velocity](#velocity)\n",
    "    * [Local navigation](#localnavigation)\n",
    "    * [Results](#MCresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "\n",
    "This project is made in the frame of the EPFL course: Basics of Mobile Robotics (BoMR). The goal is to program a robot, the Thymio, to go from a specific point to another while avoiding the obstacles and surviving some kidnapping sessions. In order to achieve this, we used computer vision, filtering, global navigation and local navigation.\n",
    "\n",
    "As you can see, we are on a car racing circuit, where our beloved thymio is racing against the clock.\n",
    "\n",
    "Our program uses an overhead camera to detect the obstacles, the track, the thymio and the finishing line. The global navigation part, anaibles to do path planning to reach the line (goal). Finally we have a Kalman filter, using the robot's poistion from the camera and odometry, allowing the thymio to know and correct its position in real time, even with a hidden camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"image\\schema-explication.png\" alt=\"Structure\" title=\"Map\" style=\"width:600px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "     \"Project structure\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"image/thymio_futur.jpeg\" alt=\"Setup Map\" title=\"Map\" style=\"width:200px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "     Our thymio in the futur by AI\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AJOUTER NOTRE VIDEO\n",
    "\n",
    "from IPython.display import Video\n",
    "\n",
    "video_path = 'chemin/vers/votre/video.mp4'\n",
    "video = Video(video_path)\n",
    "\n",
    "#affiche la vidéo\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the camera is on, different options can be displayed, by pressing the following keys: \n",
    "* p: Quits the program.\n",
    "\n",
    "The camera requires around one minute to launch. The terminal will show you the state of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Global set-up <a class=\"anchor\" id=\"computervision\"></a>\n",
    "\n",
    "### Map Set-up <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "The road is in white and the obstacles are in black. We have in addition some white 3D obstacles for local avoidance, not on the picture. The elbow obstacle is turned 180° to suit our mood at the time. The demo works both ways.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"image/setup_map.jpg\" alt=\"Setup Map\" title=\"Map\" style=\"width:400px;\"/> \n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "     Map\n",
    "</div>\n",
    "\n",
    "### Libraries & Constants<a class=\"anchor\" id=\"libraries\"></a>\n",
    "\n",
    "\n",
    "Using Numpy, OpenCv and Shapely libraries, we get a livestream from the webcam and we use the 20th frame of the video to detect most of the things that we need to plan the shortest path for the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from tdmclient import ClientAsync, aw\n",
    "import time\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "import threading\n",
    "import keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "DEFAULT_GRID_RES = 19 #resolution of the grid in pixels\n",
    "TIMESTEP = 0.075 #time between each iteration of the main loop\n",
    "SIZE_THYM = 2.5  #size of thymio in number of grid\n",
    "SPEEDCONV = 0.05\n",
    "BASESPEED = 50\n",
    "LOST_TRESH = 6 #treshold to be considered lost\n",
    "REACH_TRESH = 3 #treshhold to reach current checkpoint\n",
    "REACH_GOAL_TRESH = 1 #treshhold to reach current checkpoint\n",
    "GLOBAL_PLANNING = True\n",
    "GOAL_REACHED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=\"red\">### EXPLAIN MEANING OF THE CONSTANTS###</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented several classes, so we can use the same variables along the entire code coming from different persons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch demonstration <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "To run our project, you need to download this [GitHub repository](https://github.com/Thebestvic07/BOMR_Project.git).\n",
    "\n",
    "To launch our demo, you need to follow those steps:\n",
    "1. Run the main.py file\n",
    "2. Don't put anything on the map except of the obstacles\n",
    "3. When \"searching thymio and goal\" is written in the terminal, you can add both\n",
    "4. A new window will show the optimal path to follow\n",
    "4. Press \"p\" to exist the demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main.py structure <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "We decided to use the different following thread in the main so several task can run in parallel.\n",
    "\n",
    "\n",
    "##### 1. Camera Thread (<span style=\"text-decoration: underline;\">*camera_thread*</span>):\n",
    "\n",
    "**Function**: run_camera\n",
    "\n",
    "**Purpose & Purpose & Execution**: The run_camera function is executed in this thread. It continuously captures frames, processes ArUco markers, and updates the robot and goal positions.\n",
    "\n",
    "##### 2. Thymio Thread (<span style=\"text-decoration: underline;\">*thymio_thread*</span>):\n",
    "    \n",
    "**Function**: update_thymio\n",
    "\n",
    "**Purpose & Execution**: The update_thymio function is executed in this thread. It continuously ($f=10s^{-1}$) reads variables from the Thymio robot, updating its state.\n",
    "\n",
    "##### 3. Display Thread (<span style=\"text-decoration: underline;\">*display_thread*</span>):\n",
    "\n",
    "**Function**: display\n",
    "\n",
    "**Purpose & Execution**: The display function is executed in this thread. It continuously updates the display based on the robot's position, goal, and the environment's state on the screen.\n",
    "\n",
    "##### 4. Main Thread:\n",
    "\n",
    "**Function**: __main__\n",
    "\n",
    "**Purpose**: The main thread that orchestrates the overall control flow.\n",
    "**Execution: The script starts by initializing various objects, threads, and constants. It then enters the main loop, which handles tasks such as Thymio initialization, path planning, motion updates, obstacle avoidance, and goal-reaching conditions. The main loop is controlled by a timer, and the script continuously monitors the state of the robot and the environment.\n",
    "\n",
    "##### 5. Notes:\n",
    "\n",
    "The **Daemon** parameter is always set to True, so when the threads closes when the main program exits.\n",
    "\n",
    "\n",
    "\n",
    "The main thread orchestrates the overall control flow, handles path planning, motion control, obstacle avoidance, and goal-reaching conditions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Vision <a class=\"anchor\" id=\"computervision\"></a>\n",
    "\n",
    "\n",
    "After trying a lot of days to do shape detections, we decided to use the aruco technic to detect the thymio, the goal.\n",
    "\n",
    "The obstacles are in black, so the thymio can go everywhere it is white.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map detection<a class=\"anchor\" id=\"mapdetection\"></a>\n",
    "\n",
    "The map detection is only done once at the begining of the timeline. We first take a single frame, with only the black ostacles and one aruco. The aruco is there so we can compute the grid resolution. It is a calibration image.\n",
    "\n",
    "The creation of the black and white image involves the is_black_cell function, which evaluates whether a grid cell in the captured image is predominantly black. This function takes an image as input and determines whether the cell contains more than 1/8 black pixels. The process includes converting the image to grayscale, creating a binary mask using a specified threshold, calculating the percentage of black pixels, and comparing it to a threshold percentage (here, set to 10%). If the percentage of black pixels surpasses the threshold, the cell is categorized as black, signifying the presence of obstacles, otherwise, it is classified as white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_black_cell(image):\n",
    "    # Know if the grid cell contains more than 1/8 pixels who are black\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_mask = cv2.threshold(image, 110, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Calculate the percentage of black pixels in the image\n",
    "    total_pixels = image.size\n",
    "    black_pixels = total_pixels - cv2.countNonZero(binary_mask)\n",
    "    percentage_black = (black_pixels / total_pixels) * 100\n",
    "\n",
    "    # Set the threshold percentage\n",
    "    threshold_percentage = 10  # Adjust this threshold as needed\n",
    "\n",
    "    # Check if the percentage of black pixels exceeds the threshold\n",
    "    if percentage_black > threshold_percentage:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to generate a simplified black and white representation of the environment, highlighting obstacle positions for further processing in Thymio navigation and obstacle avoidance.\n",
    "\n",
    "The map is created when the following line is called in the main. It will store the builtmap as an image \"capture_frame.jpq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builtmap, grid_res = apply_grid_to_camera(DEFAULT_GRID_RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"image\\captured_frame.png\" alt=\"Setup Map\" title=\"Map\" style=\"width:400px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "     \"captured_frame.png\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aruco detection<a class=\"anchor\" id=\"arucodetection\"></a>\n",
    "\n",
    "The Aruco markers are detected using the Aruco marker detection functionality provided by the OpenCV library. The cv2.aruco.getPredefinedDictionary function is used to obtain a predefined Aruco marker dictionary, and cv2.aruco.DetectorParameters is employed to configure the marker detection parameters. The cv2.aruco.detectMarkers function is then applied to find and extract Aruco markers' corners and IDs in the camera frame.\n",
    "\n",
    "We will use those informations to get the positions of the Thymio and the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_arucos(corners, ids, image):\n",
    "\n",
    "    arucos = []\n",
    "    if len(corners) > 0:\n",
    " \n",
    "        ids = ids.flatten()\n",
    " \n",
    "        for (markerCorner, markerID) in zip(corners, ids):\n",
    "            corners = markerCorner.reshape((4, 2))\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = corners\n",
    " \n",
    "            topRight = (int(topRight[0]), int(topRight[1]))\n",
    "            bottomRight = (int(bottomRight[0]), int(bottomRight[1]))\n",
    "            bottomLeft = (int(bottomLeft[0]), int(bottomLeft[1]))\n",
    "            topLeft = (int(topLeft[0]), int(topLeft[1]))\n",
    " \n",
    "            cX = int((topLeft[0] + bottomRight[0]) / 2.0)\n",
    "            cY = int((topLeft[1] + bottomRight[1]) / 2.0)\n",
    "            \n",
    "            arucos.append((cX, cY, markerID, (bottomLeft, topLeft)))\n",
    "            \n",
    " \n",
    "    return image, arucos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thymio detection<a class=\"anchor\" id=\"thymiodetection\"></a>\n",
    "\n",
    "The ArUco marker 95 is placed on the Thymio. \n",
    "\n",
    "The ArUco identification through the *show_robot* function, gives us relevant information such as the center coordinates and orientation of the Thymio.\n",
    "\n",
    "If the Thymio is not detected (hidden camera for example), it will return a (0,0) position. So the kalman function can continue working without knowing the current localization of the Thymio.\n",
    "\n",
    "The choice to return this position is due that our reference ArUcO is based at this localisation. So when we are doing the obstacle map, it will be considered such as being an obstacle. The Thymio cannot be at (0,0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_robot(frame, arucos, grid_resolution):\n",
    "    real_center = robot_center_is(arucos)\n",
    "    robot_pos = center_in_grid(arucos, 95, grid_resolution)\n",
    "    angle = get_angle_of_robot(arucos)\n",
    "    frame = draw_arrow(frame, arucos, angle)\n",
    "\n",
    "    return frame, arucos, robot_pos, angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal detection <a class=\"anchor\" id=\"goaldetection\"></a>\n",
    "\n",
    "The goal detection process relies on another ArUco marker (n°99) specifically placed to represent the goal location. Similar to Thymio detection, the ArUco marker for the goal is identified in the captured frame. The center coordinates of this marker are extracted to determine the goal's position in the grid. To know its position, we only need to get the center of the ArUcO n°99 by calling this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal_pos = center_in_grid(arucos, 99, grid_res)\n",
    "\n",
    "def center_in_grid(arucos, id, grid_resolution):\n",
    "    if len(arucos) !=0:\n",
    "        for i in range(len(arucos)):\n",
    "            if arucos[i][2] == id:\n",
    "                x = int(arucos[i][0]/grid_resolution)\n",
    "                y = int(arucos[i][1]/grid_resolution)\n",
    "                return (x, y)\n",
    "    return (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable \"last_known_goal_pos\" is updated only when the measure goal_position is diffenrent of (0,0). We consider that the goal will not change of position if the camera is hidden. Therefore we are working with the \"last_known_goal_pos\" variable in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obstacles detection <a class=\"anchor\" id=\"obstaclesdetection\"></a>\n",
    "\n",
    "As explained above, the obstacle detection is based on the assumption that non-white cells in the environment are representing obstacles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Camera Thread <a class=\"anchor\" id=\"camerathread\"></a>\n",
    "\n",
    "In the camera thread function *run_camera*, the position of the Thymio and the Goal are only updated when they are none zeros.\n",
    "\n",
    "Both cannot be located in the (0,0) position in our map, because it is an obstacle. Therefore we know that if we get (0,0) as a position, it means we cannot detect it with the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_camera(mes_pos : Robot, mes_goal: Point, camera : Camera, grid_res=DEFAULT_GRID_RES):\n",
    "    '''\n",
    "    Function that updates the global Mes_Robot variable with camera data every 0.1 seconds on average\n",
    "\n",
    "    '''\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    camera.update(ret)\n",
    "    \n",
    "    width, height = 707, 10007\n",
    "    # width, height = set_param_frame(arucos)\n",
    "\n",
    "    last_known_goal_pos = (0, 0)\n",
    "    robot_pos = (0, 0)\n",
    "    while True:\n",
    "        if cap.isOpened() == False:\n",
    "            print(\"Error : video stream closed\")\n",
    "        else:\n",
    "            frame = cap.read()[1]\n",
    "\n",
    "            frame, arucos = get_arucos(frame)\n",
    "\n",
    "            frame, arucos, robot_pos, angle = show_robot(frame, arucos, grid_res)\n",
    "            goal_pos = center_in_grid(arucos, 99, grid_res)\n",
    "\n",
    "            if robot_pos != (0,0) :\n",
    "                robot_pos = Point(robot_pos[0], robot_pos[1])\n",
    "                mes_pos.update(Robot(robot_pos, angle, True))\n",
    "            else:\n",
    "                mes_pos.update(Robot(mes_pos.position, mes_pos.direction, False))\n",
    "                    \n",
    "            if goal_pos != (0, 0):\n",
    "                mes_goal.update(Point(goal_pos[0], goal_pos[1]))\n",
    "            draw_goal(frame, arucos, grid_res)\n",
    "\n",
    "            # frame = projected_image(frame, arucos, width, height)\n",
    "            cv2.imshow(\"Video Stream\", frame)\n",
    "\n",
    "            # print(f'Robot position: {robot_pos} and angle: {angle}')\n",
    "            # print(f'Goal position: {goal_pos}')\n",
    "            \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        time.sleep(0.095)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results <a class=\"anchor\" id=\"results\"></a>\n",
    "\n",
    "The results are in the majority of the time good. Again we mostly have problem with the time it takes the camera to connect to the computer. Therefore we put waiting nodes, to let everything get in place before launching the \"race\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the vision, you can run the file **test_thread_cam.py\". It will show the obstacle map, the localization of the Thymio and the goal.\n",
    "\n",
    "Press on 'p' to stop the videos, and kill the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Path planning<a class=\"anchor\" id=\"pathplanning\"></a>\n",
    "\n",
    "The goal of the path planning is to find the fastest, so the shortest, path for Thymio to reach the final line. It takes as input the visibility graph of the map, and returns the optimal path, which is a list of coordinates for Santo to follow. \n",
    "\n",
    "Where do we get our informations ?\n",
    "\n",
    "Thanks to the previous steps, we get a global map composed of white and black cells (\"capture_frame.jpg\") that enables us to know where the thymio can go through.\n",
    "\n",
    "The start and end positions are defined by the initial position of the Thymio (aruco 95) and the goal (aruco 99) last detected position \"last_known_goal_pos\".\n",
    "\n",
    "The Path Planning algorithm used was inspired by the A* algorithm seen in the course excercise session $^{[1]}$.\n",
    "\n",
    "Path Following Module: the estimation of the robot's position is given by the kalman_filter function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global navigation <a class=\"anchor\" id=\"globalnavigation\"></a>\n",
    "\n",
    "We decided to us the A* algorithm for the path planning. The goal is to find the optimal path from a starting point to a goal point, considering obstacles in the environment.\n",
    "\n",
    "Prior computing the optimal path, we had a half thymio length to all obstacle sides, so that the Thymio, considered as a point, want run in it. Now we can calculate the path using the A* algorithm,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A* algorithm <a class=\"anchor\" id=\"Aalgorithm\"></a>\n",
    "\n",
    "\n",
    "We chose to use the A* algorithm because it optimizes the search of the optimal path by expanding the most promising node chosen according some cost and heuristic functions.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"image\\A-algorithm-explanation.png\" alt=\"A algo slide\" title=\"A * algorithm\" style=\"width:400px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    \"A* algorithm, extending Dijkstra\" [1]\n",
    "</div>\n",
    "\n",
    "Functions:\n",
    "\n",
    "$f(n)=g(n)+h(n)$\n",
    "\n",
    "$f(n)$: The f-score of node nn\n",
    "\n",
    "$g(n)$: The cost of the cheapest path from the start node to node nn\n",
    "\n",
    "$h(n)$: The heuristic estimate of the cost from node nn to the goal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *calculate_path* function orchestrates the entire path planning process presented below:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **Initialization:**\n",
    "    The algorithm starts by checking if the start and goal points are within the boundaries of the map and whether they are in a traversable space. If not, an exception is raised.\n",
    "    The possible movements are defined based on 8-connectivity, meaning the algorithm considers movements in eight possible directions (horizontal, vertical, and diagonal).\n",
    "    The A* algorithm uses sets (openSet and closedSet) and dictionaries (cameFrom, gScore, and fScore) to keep track of nodes and their associated scores.\n",
    "    \n",
    "    \n",
    "    For node n, cameFrom[n] is the node immediately preceding it on the cheapest path from start to n currently known.\n",
    "\n",
    "    For node n, gScore[n] is the cost of the cheapest path from start to n currently known.\n",
    "    \n",
    "    For node n, fScore[n] := gScore[n] + h(n). map with default value of Infinity\n",
    "\n",
    "#### Implementation <a class=\"anchor\" id=\"Aalgorithm\"></a>\n",
    "\n",
    "\n",
    "* **A-Algorithm Implementation:**\n",
    "    The A* algorithm is implemented using a while loop that continues until there are no more nodes to explore (openSet is empty) or the goal is reached.\n",
    "    In each iteration, the algorithm selects the node with the lowest fScore from the openSet. This node is set as the current node.\n",
    "    The neighbors of the current node are explored, and for each neighbor, the algorithm calculates tentative scores (gScore and fScore).\n",
    "    If a neighbor is not in the openSet, it is added. If the tentative gScore for a neighbor is lower than its current gScore, the algorithm updates the scores and backtracks the optimal path.\n",
    "    The algorithm continues to explore nodes until the goal is reached or all reachable nodes have been explored.\n",
    "\n",
    "* **Reconstruction of Path:**\n",
    "    If the goal is reached, the *reconstruct_path* function is called to reconstruct the optimal path from the start to the goal using the information stored in the cameFrom dictionary.\n",
    "    The reconstructed path is returned along with the set of nodes that were visited during the algorithm (closedSet).\n",
    "\n",
    "* **Visualization (Optional):**\n",
    "    The code includes optional visualization elements, such as displaying the visited nodes with a time delay 0f 0.05 seconds, to visualize the algorithm's progress.\n",
    "\n",
    "* **Error Handling:**\n",
    "    If the open set becomes empty and the goal has not been reached, a message is printed indicating that no path was found to the goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_path(env, finalpath, extended_obs, visitedNodes, size_thym, PLOT=False):\n",
    "    \"\"\"calls all functions to calculate path\n",
    "       out: path given in a list of Points \n",
    "    \"\"\"\n",
    "\n",
    "    grid, start, goal = create_grid(env)\n",
    "    occupancy_grid = map_without_collision(grid, extended_obs, size_thym)\n",
    "    max_x, max_y = grid.shape[0], grid.shape[1] # Size of the map\n",
    "    # List of all coordinates in the grid\n",
    "    x,y = np.mgrid[0:max_x:1, 0:max_y:1]\n",
    "    pos = np.empty(x.shape + (2,))\n",
    "    pos[:, :, 0] = x; pos[:, :, 1] = y\n",
    "    pos = np.reshape(pos, (x.shape[0]*x.shape[1], 2))\n",
    "    coords = list([(int(x[0]), int(x[1])) for x in pos])\n",
    "\n",
    "    # Define the heuristic, here = distance to goal ignoring obstacles\n",
    "    h = np.linalg.norm(pos - goal, axis=-1)\n",
    "    h = dict(zip(coords, h))\n",
    "\n",
    "    # Run the A* algorithm\n",
    "    path, closedSet = A_Star(start, goal, h, coords, occupancy_grid, visitedNodes)\n",
    "    path = np.array(path).reshape(-1, 2).transpose()\n",
    "\n",
    "    closedSet = np.array(closedSet).reshape(-1, 2).transpose()\n",
    "\n",
    "    convert_path(path, finalpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results <a class=\"anchor\" id=\"A*CCL*\"></a>\n",
    "\n",
    "We are happy with the results of our algorithm. It can very well compute the optimal path through the map. \n",
    "The main limit we can see is the time it takes to compute it: if the goal is far away, we need to wait a bit.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"image\\map_path_compute.jpg\" alt=\"pathcomputation\" title=\"Results path computation\" style=\"width:750px;\"/> \n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "     \"Results of the optimal path computation\"\n",
    "</div>\n",
    "\n",
    " \n",
    "The goal is labeled in red, and our thymio has a green arrow on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "\n",
    "# Kalmann Filtering <a class=\"anchor\" id=\"filtering\"></a>\n",
    "\n",
    "We decided to work with a Kalman filter because it is optimal for the robot localization due to its ability to handle noise and uncertainties effectively.\n",
    "\n",
    "Kalman filtering is used to estimate the state of a system based on a series of noisy and incomplete measurements. In the context of robotics and localization, Kalman filters are particularly beneficial for obtaining a good position estimation of a robot. The state of the robot is defined as its position, angle, angular velocity and velocity.\n",
    "\n",
    "We need to gather informations about the thymio's detected state:\n",
    "- overhead camera localization\n",
    "- odometry and thymio's target speed\n",
    "\n",
    "Furthermore, Kalman filters use a recursive estimation process, meaning that they continually update the state estimate as new measurements become available. This allows the filter to provide real-time position estimates, even if the camera is hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical model <a class=\"anchor\" id=\"mathmodel\"></a>\n",
    "\n",
    "The Thymio's movement is describe by its translation on the x and y axes and its rotation $\\theta$. \n",
    "It is ruled by the following motion laws :\n",
    "\n",
    "1. Forward speed :    $v = K*\\frac{dmot_R + dmot_L}{2}$\n",
    "2. Rotational speed : $w = K*\\frac{dmot_R - dmot_L}{\\pi * THYMIO\\_WIDTH}$\n",
    "3. Position :         $x = v \\cdot \\cos(\\theta) \\cdot dt$\n",
    "4. Position :         $y = v \\cdot \\sin(\\theta) \\cdot dt$\n",
    "5. Direction :        $\\theta = w \\cdot dt$\n",
    "\n",
    "(*_K being a conversion factor such that 1 thymio's speed equals K x 1 unit per second (our unit being a grid case))\n",
    "\n",
    "Our model is thus non linear due to theta !\n",
    " \n",
    "\n",
    "$$X = \n",
    "\\begin{bmatrix}\n",
    "v\\\\\n",
    "\\omega\\\\\n",
    "p_x\\\\\n",
    "p_y\\\\\n",
    "\\theta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where:\n",
    "- $v$ is the magnitude speed in $[pxl/s]$.\n",
    "- $\\omega$ is the angular speed $[rad/s]$.\n",
    "- $p_x$, $p_y$ is the cartesian position in $[pxl]$.\n",
    "- $\\theta$ is the orientation in $[rad]$.\n",
    "\n",
    "\n",
    "         | \n",
    "\n",
    "Our input is $U = [dmot_l, dmot_r]$, the delta between the new and the previous input motor speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on filtering <a class=\"anchor\" id=\"filterconcl\"></a>\n",
    "\n",
    "The extended Kalman filter satifies our needs for this project. It allows to merge the camera measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "## Motion Control <a class=\"anchor\" id=\"motioncontrol\"></a>\n",
    "\n",
    "To compute the speed of the Thymio, we decided to decompose the computation in three parts: orientation, position and obstacle detection.\n",
    "The final velocity will be composed of the three different speeds: $\\vec{v}_{total}=\\vec{v}_{orientation}+\\vec{v}_{forward}+\\vec{v}_{obstacle}= \\begin{bmatrix} v_L \\\\ v_R \\end{bmatrix}$\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"image/motion_control.png\" alt=\"Motion Control\" title=\"A robot is moving around with the proposed motion framework\" \n",
    "    style=\"width:300px;\"/>\n",
    "        <img src=\"image/graph_motion.jpg\" alt=\"Motion Graph\" title=\"Graph motion\" style=\"width:291px;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "   Left: \"A robot is moving around with the proposed motion framework\" [2]\n",
    "</div> \n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "   Right: Angle and major points in our Map\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orientation & angle error <a class=\"anchor\" id=\"velorientation\"> </a>\n",
    "\n",
    "We define orientation changes as an angle error defined by: $\\alpha _E= \\alpha _C - \\alpha _T$, where $\\alpha _T$ is the current angle of the thymio and $\\alpha _C$ is the desired angle change.\n",
    "\n",
    "With classic trigonometry formel we get that: $\\alpha _C = arctan(\\frac{P_{y+1}-P_{yT}}{P_{x+1}-P_{xT}})$, where $P_T$ the position of the Thymio and $P_{+1}$ the desired position of the Thymio.\n",
    "\n",
    "\n",
    "One of the big limitations found is because of the use of the angles functions. Indeed, it gives values between $[0; 2\\pi]$. So if in some cases we where to near of 0 or $2\\pi$, the robot just turns around infinitly. \n",
    "Therefore we decided to normalize all angles between $[-\\pi; \\pi]$, to prevent discontinuities and unexpected behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_angle_error(position, checkpoint, thymio_direction):\n",
    "  \"\"\"\n",
    "  Computes the angle error between the robot and the goal.\n",
    "\n",
    "  :param position: Current position of the robot.\n",
    "  :param checkpoint: position to reach (next point in the global path).\n",
    "  :param thymio_angle: Current orientation of the robot.\n",
    "  :return: Angle error between the robot and the goal.\n",
    "  \"\"\"\n",
    "  angle = np.arctan2( (checkpoint.y - position.y), (checkpoint.x - position.x))\n",
    "  # angle = (angle + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "  thymio_direction = (-thymio_direction + np.pi) % (2 * np.pi) - np.pi    ## + thymio_direction because it's defined in the negative sense of trigo\n",
    "\n",
    "  angle_error = angle - thymio_direction         \n",
    "  angle_error = (angle_error + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "  print(\"angle: \", angle, \"thymio_direction: \", thymio_direction, \"angle_error: \", angle_error)\n",
    "\n",
    "  return angle_error\n",
    "\n",
    "\n",
    "\n",
    "def compute_derived_angle_error(angle_error, prev_angle_error, dt):\n",
    "  \"\"\"\n",
    "  Computes the derived angle error between the robot and the goal.\n",
    "\n",
    "  :param angle_error: Angle error between the robot and the goal.\n",
    "  :param prev_angle_error: Previous angle error between the robot and the goal.\n",
    "  :param dt: Time step.\n",
    "  :return: Derived angle error between the robot and the goal.\n",
    "  \"\"\"\n",
    "  return (angle_error - prev_angle_error) / dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PD Controler<a class=\"anchor\" id=\"controler\"> </a>\n",
    "\n",
    "We decided to use a PD controler because of it is simple to implement and it is suitable for fast, real-time feedback.\n",
    "\n",
    "First, we only had a $K_p$ controler, we could see that the Thymio made some oscillations in its motion, therefore we added the $K_d$ to counteract those undesired movement and so get a more stable response. The derivative helps to reduce the sensitivity of the noise.\n",
    "\n",
    "$K_{p}$ and $K_d$ are constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute velocity <a class=\"anchor\" id=\"veldirection\"> </a>\n",
    "\n",
    "\n",
    "* *'base_speed'* is the forward velocity that you want the robot to maintain.\n",
    "\n",
    "$v_{forward}=const.$\n",
    "\n",
    "The other calculated speeds are going to act as a penalty or gain on top of the *'base_speed'* for each wheel. \n",
    "\n",
    "* *'rot_speed'* is the rotational speed calculated using the PD controler based on the angle error (*'angle_error'*) and its derivative (*'derived_angle_error'*).\n",
    "\n",
    "$\\vec{v}_{orientation}=(K_{p} * \\alpha _E + K_{d} * \\frac{d \\alpha _E}{dt} )\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$.\n",
    "\n",
    "The [1,-1] matrix is because to turn, the two wheels need to have opposite speeds.\n",
    "\n",
    "\n",
    "$\\vec{v}_{wheel}= \\vec{v}_{forward} + \\vec{v}_{orientation}$\n",
    "\n",
    "\n",
    "The rotational speed is clipped to avoid saturation (>500). If the absolute value of rot_speed exceeds the limit 200 - base_speed, it is adjusted to this limit to prevent oversaturation and the program to crash.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controller(robot, checkpoint, base_speed, prev_angle_error, dt, final_goal_reached=False):\n",
    "  '''\n",
    "  This function computes the motor velocities based on the position, goal, and control parameters.\n",
    "  param robot : Current position of the robot\n",
    "  param checkpoint : Goal position to reach (next point in the global path)\n",
    "  param base_speed : Base speed of the robot\n",
    "  param final_goal_reached : Flag indicating whether the final goal is reached, not motion is needed.\n",
    "\n",
    "  return : Input as a tuple (MotL, MotR) \n",
    "  return : angle_error\n",
    "  '''\n",
    "  if final_goal_reached:\n",
    "      return 0, 0, 0\n",
    "  \n",
    "  angle_error = compute_angle_error(robot.position, checkpoint, robot.direction)\n",
    "  derived_angle_error = compute_derived_angle_error(angle_error, prev_angle_error, dt)\n",
    "\n",
    "  rot_speed = Kp * angle_error + Kd * derived_angle_error\n",
    "\n",
    "  # Clip the rotation speed to avoid saturation\n",
    "  rot_speed = rot_speed if abs(rot_speed) < 200 - base_speed else 200 - base_speed\n",
    "\n",
    "  MotL = base_speed + rot_speed\n",
    "  MotR = base_speed - rot_speed\n",
    "\n",
    "  return MotL, MotR, angle_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local navigation <a class=\"anchor\" id=\"localnavigation\"> </a>\n",
    "\n",
    "First we wanted to use the potential field Navigation system and totaly ignoring the other velocities when an obstacle is encountered. It came quickly out that the thymio easily gets out of the desired path. Therefore, we tried to do something similar, but the $\\vec{v}_{obstacle}$ will act as a gain on the total computed speed $\\vec{v}_{wheel}$. We used the same gains for the proximity sensors as in the exercise session 4 $^{[1]}$.\n",
    "\n",
    "The external sensors have a stronger weight than the internal ones. The Thymio will be 'repulsed' more quickly when it detects an obstacle, so it will no longer be in line with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is similar as the one used in the merge version of the code, to make it work independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if needed decomment:\n",
    "\n",
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_python\n",
    "timer_period[0] = 10  # 10ms sampling time\n",
    "\n",
    "@onevent \n",
    "def timer0():\n",
    "    global prox_horizontal, motor_left_target, motor_right_target\n",
    "    speed0 = 75     # nominal speed\n",
    "\n",
    "    # gains used with front proximity sensors [0,5]\n",
    "    weights = [6, 4, -3, -6, -8]\n",
    "\n",
    "    addLeft=0\n",
    "    addRight=0\n",
    "    diffDelta=0\n",
    "    # adjustment for obstacles, \"gradient\" due to obstacles\n",
    "    for i in range(5):\n",
    "        addLeft += prox_horizontal[i] * weights[i]//200\n",
    "        addRight += prox_horizontal[i] * weights[4 - i]//200\n",
    "    \n",
    "    motor_left_target = speed0 + addLeft\n",
    "    motor_right_target = speed0 + addRight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "<video width=\"640\" height=\"360\" controls autoplay>\n",
    "  <source src=\"image/local_avoidance.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results <a class=\"anchor\" id=\"MCresults\"> </a>\n",
    "\n",
    "First we wanted to use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Conclusion <a class=\"anchor\" id=\"conclusion\">\n",
    "\n",
    "To conclude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources :\n",
    "\n",
    "\n",
    "[1] MICRO-452 \"Basics of mobile robotics\" course of EPFL from Professor Francesco Mondada\n",
    "\n",
    "[2] « Kinematics — CoopRobo 1.0.0 documentation » https://cooprobo.readthedocs.io/en/latest/mobile/pioneer/model/kinematics.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
