{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MICRO-452 Basics of mobile robotics\n",
    "\n",
    "## Project Report\n",
    "\n",
    "Team  13:\n",
    "\n",
    "Wilhelm Laetitia,\n",
    "Beuret Sylvain,\n",
    "Labbe Victor,\n",
    "Jaffal Oussama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [Global Set-up](#globalsetup)\n",
    "    * [Map Set-up](#mapsetup)\n",
    "    * [Libraries & constants](#libraries)\n",
    "    * [Launch demo](#demo)\n",
    "    * [Main.py structure](#main) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [Computer vision](#computervision)\n",
    "    * [Map detection](#mapdetection)\n",
    "    * [ArUco detection](#arucodetection)\n",
    "    * [Obstacles detection](#obstaclesdetection)\n",
    "    * [Projection image](#projected-image)\n",
    "    * [Results](#CVresults)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [Path planning](#pathplanning)\n",
    "    * [Global Navigation](#globalnavigation)\n",
    "    * [A* algorithm](#Aalgorithm)\n",
    "    * [Implementation](#implementation)\n",
    "    * [Results](#PPresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [Kalman Filtering](#filtering)\n",
    "    * [General, filtering](#genfiltering)\n",
    "    * [Mathematical model](#mathmodel)\n",
    "    * [Filter](#filter)\n",
    "    * [Results](#Kresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. [Motion control](#motioncontrol)\n",
    "    * [Orientation & angle error](#orientation)\n",
    "    * [PD controler](#controler)\n",
    "    * [Compute velocity](#velocity)\n",
    "    * [Local navigation](#localnavigation)\n",
    "    * [Results](#MCresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "\n",
    "This project is made in the frame of the EPFL course: Basics of Mobile Robotics (BoMR). The goal is to program a robot, the Thymio, to go from a specific point to another while avoiding the obstacles and surviving some kidnapping sessions. In order to achieve this, we used computer vision, filtering, global navigation and local navigation.\n",
    "\n",
    "As you can see, we are on a car racing circuit, where our beloved thymio is racing against the clock.\n",
    "\n",
    "Our program uses an overhead camera to detect the obstacles, the track, the thymio and the finishing line. The global navigation part, anaibles to do path planning to reach the line (goal). Finally we have a Kalman filter, using the robot's position from the camera and odometry, allowing the thymio to know and correct its position in real time, even with a hidden camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"image\\schema-explication.jpeg\" alt=\"Structure\" title=\"Map\" style=\"width:800px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "     \"Project structure\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"image/thymio_futur.jpeg\" alt=\"Setup Map\" title=\"Map\" style=\"width:200px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "     Our thymio in the futur by AI\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The videos are to big, so please refer to this one drive file to see our final results. https://1drv.ms/f/s!AvyzZel2mVd0qTD84Sf01JUx_LqD?e=luYhZV  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the camera is on, a window is displayed.\n",
    "To shut it down press the key 'q' when having the mouse on it BUT be careful as it will shut down the thread that might be feeding the main with data\n",
    "\n",
    "The camera requires quite some time to launch. The terminal will provide you with the state of the process during all time !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Global set-up <a class=\"anchor\" id=\"computervision\"></a>\n",
    "\n",
    "### Map Set-up <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "Our environment as be designed to ressemble a (basic) Mario Kart circuit. \n",
    "The road is in white and the sideways are in black. We have in addition (not displayed on picture) some 3D obstacles i.e. bad turtles staying on the road that we will need to avoid.\n",
    "The 'L' obstacle can be turned to suit our mood. The demo works any ways.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"image/setup_map.jpg\" alt=\"Setup Map\" title=\"Map\" style=\"width:400px;\"/> \n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "     Map seene from the camera\n",
    "</div>\n",
    "\n",
    "\n",
    "### Libraries <a class=\"anchor\" id=\"libraries\"></a>\n",
    "\n",
    "Using OpenCv libraries, we get a livestream from the webcam in real time. We also use an initial frame of the video to draw the map that we need to plan the shortest path for the robot.\n",
    "\n",
    "We also use librairies such as threading to achieve parallel computing of our codes, numpy for most mathematical operations and keyboard to manage our script.\n",
    "\n",
    "Finally we use ClientASync from tdmclient to communication asynchronously with our Thymio. \n",
    "\n",
    "We have also implemented several classes to facilitate the integration of the different parts of our project: \n",
    "- Most objects our our simulation have a dedicated dataclass in the file utils/data.py (such as Map, Robot, Motors, etc). We used the library dataclasses in order to define those\n",
    "- We also defined a Thymio class to interface communication with our robot in the file utils/communication.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from tdmclient import ClientAsync, aw\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "import threading\n",
    "import keyboard\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch demonstration <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "To run our project, you need to download this [GitHub repository](https://github.com/Thebestvic07/BOMR_Project.git).\n",
    "\n",
    "To launch our demo, you need to follow those steps:\n",
    "1. Run the main.py file\n",
    "2. First thing the script will do is connect to the Thymio (should print in the terminal)\n",
    "3. Once connected to Thymio, it will draw and calibrate the map. *!! Don't put anything on the map except of the obstacles !!* \n",
    "4. Once this is done (should take max 1 min), the drawn map will appear in a window \n",
    "5. The script will then initialize the camera to provide real time feed in a second window (this can take a few minutes...)\n",
    "6. When \"searching thymio and goal\" is written in the terminal, you can add both + some 3D turtles for the Thymio to avoid\n",
    "7. Most objects present on the video will also be displayed on the map display alongside with some interesting features such as the optimal path to the goal \n",
    "8. Press \"esc\" in the terminal to end the demo and 'q' on the displayed windows to shut them down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main.py structure <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "We decided to use the different following thread in the main so several task can run in parallel.\n",
    "\n",
    "\n",
    "##### 1. Camera Thread (<span style=\"text-decoration: underline;\">*camera_thread*</span>):\n",
    "\n",
    "**Function**: run_camera\n",
    "\n",
    "**Purpose & Execution**: The run_camera function is executed in this thread. It continuously captures frames ($f=10Hz$), processes ArUco markers, and updates the robot and goal positions.\n",
    "\n",
    "##### 2. Thymio Thread (<span style=\"text-decoration: underline;\">*thymio_thread*</span>):\n",
    "    \n",
    "**Function**: update_thymio\n",
    "\n",
    "**Purpose & Execution**: The update_thymio function is executed in this thread. It continuously reads motors variables ($f=10Hz$) from the Thymio, updating its state.\n",
    "\n",
    "##### 3. Display Thread (<span style=\"text-decoration: underline;\">*display_thread*</span>):\n",
    "\n",
    "**Function**: display\n",
    "\n",
    "**Purpose & Execution**: The display function is executed in this thread. It continuously updates the map displayed on a side window based on the environment's state.\n",
    "\n",
    "##### 4. Main Thread:\n",
    "\n",
    "**Function**: __main__\n",
    "\n",
    "**Purpose**: The main thread orchestrates the overall control flow.\n",
    "\n",
    "**Execution**: The script starts by initializing various objects, threads, and constants. It then enters the main loop, which handles tasks such as state estimation, path planning, motion updates, obstacle avoidance, and goal-reaching conditions. The main loop is controlled by a timer, and the script continuously monitors the state of the robot and the environment.\n",
    "\n",
    "##### Note: The various Threads are so-called **Daemon threads**, so when the main program exits, the threads closes with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Vision <a class=\"anchor\" id=\"computervision\"></a>\n",
    "\n",
    "\n",
    "After trying a lot of days to do shape detections, we decided to use Aruco markers (sub-library of OpenCV) to detect the thymio, and the goal. It proved to work a lot better than shape detection, especially in various light conditions. \n",
    "\n",
    "The obstacles are in black, so the thymio can go everywhere it is white.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map detection<a class=\"anchor\" id=\"mapdetection\"></a>\n",
    "\n",
    "The map detection is performed only once at the beginning of the process. We capture a single calibrating frame, with only the black ostacles and one ArUco. The ArUco marker serves the purpose of computing the length of one of its sides and store its value as the grid resolution, this way the map can be calibrated, before continuing the process, to ensure that the map's resolution remains independent of the height or angle of the camera. The frame is then used as a support for the display of the environment. This frame can be found in the Project repository : 'captured_frame.png'\n",
    "\n",
    "The creation of the black and white image involves the is_black_cell function, which evaluates whether a grid cell in the captured image is predominantly black. This function takes an image as input and determines whether the cell is considered an obstacle or not. The process includes converting the image to grayscale, creating a binary mask using a specified threshold, calculating the percentage of black pixels, and comparing it to a threshold percentage (here, set to 10% to account for conditions of test environment). If the percentage of black pixels surpasses the threshold, the cell is categorized as black, signifying the presence of obstacles, otherwise, it is classified as white.\n",
    "\n",
    "\n",
    "This process is done for each cell, and then we reconstruct the map using apply_grid method, by putting either black or white cells, and stack them horizontally and vertically, the purpose is to generate a simplified black and white representation of the environment, highlighting obstacle positions for further processing in Thymio navigation and obstacle avoidance. We also add various information in the according lists to have all the necessary data we need for the steps that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_black_cell(image):\n",
    "    # Know if the grid cell contains more than 1/8 pixels who are black\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_mask = cv2.threshold(image, 110, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Calculate the percentage of black pixels in the image\n",
    "    total_pixels = image.size\n",
    "    black_pixels = total_pixels - cv2.countNonZero(binary_mask)\n",
    "    percentage_black = (black_pixels / total_pixels) * 100\n",
    "\n",
    "    # Set the threshold percentage\n",
    "    threshold_percentage = 10  # Adjust this threshold as needed\n",
    "\n",
    "    # Check if the percentage of black pixels exceeds the threshold\n",
    "    if percentage_black > threshold_percentage:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Build a map from an image\n",
    "def apply_grid(image, grid_resolution):\n",
    "\n",
    "    # Get image dimensions\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Create black and white images\n",
    "    create_black_image(grid_resolution)\n",
    "    create_white_image(grid_resolution)\n",
    "\n",
    "    # Get resolution of the grid\n",
    "    x_cells = int(width / grid_resolution)\n",
    "    y_cells = int(height / grid_resolution)\n",
    "\n",
    "    # Creating a 2D list of grid cells\n",
    "    map = [[0 for _ in range(y_cells)] for _ in range(x_cells)]\n",
    "    obstacles = []\n",
    "    internal_map = [[0 for _ in range(y_cells)] for _ in range(x_cells)]\n",
    "\n",
    "    # Initializing variables\n",
    "    obstacles = []\n",
    "    new_image = list(range(y_cells))\n",
    "    final_image = list(range(x_cells))\n",
    "\n",
    "    for y in range(y_cells):\n",
    "        for x in range(x_cells):\n",
    "            # Define the bounding box for the current grid cell\n",
    "            x_min, y_min = x*grid_resolution, y*grid_resolution\n",
    "            x_max, y_max = x_min + grid_resolution, y_min + grid_resolution\n",
    "\n",
    "            # Crop the image to the bounding box\n",
    "            cell_content = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            # Check if the cell is an obstacle\n",
    "            if is_black_cell(cell_content):\n",
    "                map[x][y] = 0\n",
    "                obstacles.append(Point(x,y))\n",
    "            else:\n",
    "                map[x][y] = 1\n",
    "\n",
    "            # Store the cell content in the grid\n",
    "            internal_map[x][y] = change_cell(cell_content)\n",
    "\n",
    "            # Add the cell in the column\n",
    "            if x==0:\n",
    "                new_image[y] = internal_map[x][y]\n",
    "            else:\n",
    "                new_image[y] = np.hstack((new_image[y], internal_map[x][y]))\n",
    "        \n",
    "        # Add the column in the image\n",
    "        if y==0:\n",
    "            final_image = new_image[y]\n",
    "\n",
    "        if y!=0:\n",
    "            final_image = np.vstack((final_image,new_image[y]))\n",
    "\n",
    "    # Create the map object\n",
    "    map = Map([Point(0,0), Point(width,0), Point(width, height), Point(0,height)], obstacles)\n",
    "    return final_image, map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map is created when the following line is called in the main (a default grid resolution is provided in case of non detection of the Aruco Marker):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builtmap, grid_res = apply_grid_to_camera(DEFAULT_GRID_RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"image\\captured_frame.png\" alt=\"Setup Map\" title=\"Map\" style=\"width:400px;\"/>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "     captured_frame.png\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ArUco detection<a class=\"anchor\" id=\"arucodetection\"></a>\n",
    "\n",
    "The Aruco markers are detected using the Aruco marker detection functionality provided by the OpenCV library. The cv2.aruco.getPredefinedDictionary function is used to obtain a predefined Aruco marker dictionary, and cv2.aruco.DetectorParameters is employed to configure the marker detection parameters. The cv2.aruco.detectMarkers function is then applied to find and extract Aruco markers' corners and IDs in the camera frame.\n",
    "\n",
    "\n",
    "Ultimately, we opted for this solution because ArUco enables reliable vision detection, even in challenging lighting situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_aruco():\n",
    "    # Set the dictionary of aruco markers to 4x4_100\n",
    "    dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_100)\n",
    "    # Set the parameters for the detector\n",
    "    parameters =  cv2.aruco.DetectorParameters()\n",
    "    # Create the detector\n",
    "    detector = cv2.aruco.ArucoDetector(dictionary, parameters)\n",
    "    return detector\n",
    "\n",
    "def get_arucos(frame):\n",
    "    detector = set_aruco()\n",
    "    markerCorners, markerIds, rejectedCandidates = detector.detectMarkers(frame)\n",
    "    frame, arucos = get_info_arucos(markerCorners, markerIds, frame)\n",
    "    return frame, arucos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thymio detection<a class=\"anchor\" id=\"thymiodetection\"></a>\n",
    "\n",
    "The ArUco marker 95 is placed on the Thymio. *(What a nice bib, Flash McQueen has the same !)*\n",
    "\n",
    "The ArUco identification gives us relevant information such as the center coordinates and orientation of the Thymio. By using the ArUco identification we can get information such as the the coordinates of the corners and label them as follow : BottomLeft, BottomRight, TopLeft, and TopRight. This allows us to get the position and orientation of the robot :\n",
    "- For the center coordinates, it is calculated using the coordinates of 2 of the 4 corners : $(Pos_{x}, Pos_{y})  = (\\frac{\\text{TopLeft}_x + \\text{BottomRight}_x}{2}, \\frac{\\text{TopLeft}_y + \\text{BottomRight}_y}{2})$\n",
    "\n",
    "\n",
    "- For the angle we use the coordinates of one of the sides of the ArUco : $\\alpha = arctan(\\frac{TopLeft_{y}-BottomLeft_{y}}{TopLeft_{x}-BottomLeft_{x}})$\n",
    "\n",
    "In the case the Thymio is not detected (hidden camera for example), it will return a (0,0) position. So the kalman function can continue working without knowing the current localization of the Thymio.\n",
    "\n",
    "The choice to return this position is due that our reference ArUcO is based at this localisation. So when we are doing the obstacle map, it will be considered such as being an obstacle. The Thymio thus cannot be at (0,0), this position is always considered \"Not detected\". This allows the kalman filter to keep working without sensing the current localization of the Thymio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_robot(frame, arucos, grid_resolution):\n",
    "    real_center = robot_center_is(arucos)\n",
    "    robot_pos = center_in_grid(arucos, 95, grid_resolution)\n",
    "    angle = get_angle_of_robot(arucos)\n",
    "    frame = draw_arrow(frame, arucos, angle)\n",
    "\n",
    "    return frame, arucos, robot_pos, angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal detection <a class=\"anchor\" id=\"goaldetection\"></a>\n",
    "\n",
    "The goal detection process relies on another ArUco marker (ID n°99) specifically placed to represent the goal location. Similar to Thymio detection, the ArUco marker for the goal is identified in the captured frame. The center coordinates of this marker are extracted to determine the goal's position in the grid.\n",
    "\n",
    "The variable \"last_known_goal_pos\" is updated only when the measure goal_position is diffenrent of (0,0). We consider that the goal will not change its position if the camera is hidden. Therefore we are working with the \"last_known_goal_pos\" variable in the next steps. This is specially useful when the thymio is near to the desired position and starts covering parts of the ArUco goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal_pos = center_in_grid(arucos, 99, grid_res)\n",
    "\n",
    "def center_in_grid(arucos, id, grid_resolution):\n",
    "    if len(arucos) !=0:\n",
    "        for i in range(len(arucos)):\n",
    "            if arucos[i][2] == id:\n",
    "                x = int(arucos[i][0]/grid_resolution)\n",
    "                y = int(arucos[i][1]/grid_resolution)\n",
    "                return (x, y)\n",
    "    return (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable \"last_known_goal_pos\" is updated only when the measure goal_position is diffenrent of (0,0). We consider that the goal will not change of position if the camera is hidden. Therefore we are working with the \"last_known_goal_pos\" variable in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obstacles detection <a class=\"anchor\" id=\"obstaclesdetection\"></a>\n",
    "\n",
    "As explained above, the obstacle detection is based on the assumption that non-white cells in the environment are representing obstacles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Camera Thread <a class=\"anchor\" id=\"camerathread\"></a>\n",
    "\n",
    "In the camera thread function *run_camera*, the position of the Thymio and the Goal are only updated when they are none zeros.\n",
    "\n",
    "Both cannot be located in the (0,0) position in our map, because it is an obstacle. Therefore we know that if we get (0,0) as a position, it means we cannot detect it with the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_camera(mes_pos : Robot, mes_goal: Point, camera : Camera, grid_res=DEFAULT_GRID_RES):\n",
    "    '''\n",
    "    Function that updates the global Mes_Robot variable with camera data every 0.1 seconds on average\n",
    "\n",
    "    '''\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    camera.update(ret)\n",
    "    \n",
    "    width, height = 707, 10007\n",
    "    # width, height = set_param_frame(arucos)\n",
    "\n",
    "    last_known_goal_pos = (0, 0)\n",
    "    robot_pos = (0, 0)\n",
    "    while True:\n",
    "        if cap.isOpened() == False:\n",
    "            print(\"Error : video stream closed\")\n",
    "        else:\n",
    "            frame = cap.read()[1]\n",
    "\n",
    "            frame, arucos = get_arucos(frame)\n",
    "\n",
    "            frame, arucos, robot_pos, angle = show_robot(frame, arucos, grid_res)\n",
    "            goal_pos = center_in_grid(arucos, 99, grid_res)\n",
    "\n",
    "            if robot_pos != (0,0) :\n",
    "                robot_pos = Point(robot_pos[0], robot_pos[1])\n",
    "                mes_pos.update(Robot(robot_pos, angle, True))\n",
    "            else:\n",
    "                mes_pos.update(Robot(mes_pos.position, mes_pos.direction, False))\n",
    "                    \n",
    "            if goal_pos != (0, 0):\n",
    "                mes_goal.update(Point(goal_pos[0], goal_pos[1]))\n",
    "            draw_goal(frame, arucos, grid_res)\n",
    "\n",
    "            # frame = projected_image(frame, arucos, width, height)\n",
    "            cv2.imshow(\"Video Stream\", frame)\n",
    "\n",
    "            # print(f'Robot position: {robot_pos} and angle: {angle}')\n",
    "            # print(f'Goal position: {goal_pos}')\n",
    "            \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        time.sleep(0.095)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection Image <a class=\"anchor\" id=\"projected-image\"></a>\n",
    "\n",
    "To have a better and more correct view of the map and detect better the positions of the Thymio and the goal, we wanted to extract these informations directly from the projected image.\n",
    "\n",
    "\n",
    "To do this, first we needed to detect the corners of the map by using 4 ArUcos with different Ids, and get set each of the corners as the center coordinates of these ArUcos. Then we would use cv2.getPerspectiveTransform to get the matric of the transformation applied to the image and then use the cv2.warpPerspective method to have the projection image.\n",
    "<br><br>\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=image/projected_image.png height=400px alt=\"Image 1\" >\n",
    "        <p>Projection Image</p>\n",
    "    </div>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=image/non_projected_image.png height=400px alt=\"Image 2\" >\n",
    "        <p>Original Image</p>\n",
    "    </div>\n",
    "</div>\n",
    "<br><br>\n",
    "As you can see above, we can access infromation about the Thymio ArUco in the original image, but not in the projection image. We are not sure why exactly it is not possible to detect the Thymio in the first image, but we suspect it is due to distortion however slightly it might seem. We tried several techniques to resolve this issue, like making sure the resulted image having the same aspect ratio as the paper we use in the origial image, but it still wouldn't work properly. Another idea was to collect the data of the thymio from the orginal image and apply the same matrix of transformation to get the coordinates in the projection image, but this was not very stable to be used in our model.\n",
    "\n",
    "In the end, we opted for a more traditional way of collecting information from the camera, but still assuring a correct way of doing so by positioning the camera well centered above the map to have the least errors in collecting the data needed.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"image/setup_map.jpg\" alt=\"Setup Map\" title=\"Map\" style=\"width:400px;\"/>\n",
    "    <p>Well adjusted camera view</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results <a class=\"anchor\" id=\"results\"></a>\n",
    "\n",
    "The results are in the majority of the time good. Our biggest issue is actually the time it takes to the camera to connect to the computer. Therefore we put waiting nodes, to let everything get in place before launching the \"race\". \n",
    "\n",
    "When the calibrating ArUco is not detected (too much noise or bad light conditions), we can observe that obstacles are not perfectly aligned on the grid (with is due to the grid resolution by default). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the vision, you can fetch and run the file **test_thread_cam.py\" in Codes. It will show the obstacle map, the localization of the Thymio and the goal.\n",
    "\n",
    "Press on 'q' to stop the videos, and kill the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Path planning<a class=\"anchor\" id=\"pathplanning\"></a>\n",
    "\n",
    "The goal of the path planning is to find the fastest, so the shortest, path for Thymio to reach the final line. It takes as input the visibility graph of the map, and returns the optimal path, which is a list of coordinates for Thymio to follow. \n",
    "\n",
    "Where do we get our informations ?\n",
    "\n",
    "Thanks to the previous steps, we get a global map composed of white and black cells (\"capture_frame.jpg\") that enables us to know where the thymio can go through.\n",
    "\n",
    "The start and end positions are defined by the initial position of the Thymio (aruco 95) and the goal (aruco 99) last detected position \"last_known_goal_pos\".\n",
    "\n",
    "The Path Planning algorithm used was inspired by the A* algorithm seen in the course excercise session $^{[1]}$.\n",
    "\n",
    "Path Following Module: the estimation of the robot's position is given by the kalman_filter function.\n",
    "\n",
    "The global navigation computation retuns a list of coordinates to reach. During the motion, we track the position and check when a checkpoint is reached. If the distance to the next one becomes bigger as a defined treshhold, we recompute the path which is done when kidnaping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global navigation <a class=\"anchor\" id=\"globalnavigation\"></a>\n",
    "\n",
    "We decided to use the A* algorithm for the path planning. The goal is to find the optimal path from a starting point to a goal point, considering obstacles in the environment.\n",
    "\n",
    "Prior computing the optimal path, we add a half thymio length to all obstacle sides, so that the Thymio, considered as a point, won't run in it. Now we can calculate the path using the A* algorithm. Because of the use of a grid, we need to add the distance as an integers which is a limit of this method as it can block some viables paths if it rounds the distance to the upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A* algorithm <a class=\"anchor\" id=\"Aalgorithm\"></a>\n",
    "\n",
    "\n",
    "We chose to use the A* algorithm because it optimizes the search of the optimal path by expanding the most promising node chosen according to some cost and heuristic functions. In this case, we use the euclidian norm between the start and goal position as we use diagonal displacements.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"image\\A-algorithm-explanation.png\" alt=\"A algo slide\" title=\"A * algorithm\" style=\"width:400px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    \"A* algorithm, extending Dijkstra\" [1]\n",
    "</div>\n",
    "\n",
    "Functions:\n",
    "\n",
    "$f(n)=g(n)+h(n)$\n",
    "\n",
    "$f(n)$: The f-score of node nn\n",
    "\n",
    "$g(n)$: The cost of the cheapest path from the start node to node nn\n",
    "\n",
    "$h(n)$: The heuristic estimate of the cost from node nn to the goal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculate_path function orchestrates the entire path planning process presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_path(env, finalpath, extended_obs, visitedNodes, size_thym, PLOT=False):\n",
    "    \"\"\"calls all functions to calculate path\n",
    "       out: path given in a list of Points \n",
    "    \"\"\"\n",
    "\n",
    "    grid, start, goal = create_grid(env)                 #Converts useful infos from environment\n",
    "    occupancy_grid = map_without_collision(grid, extended_obs, size_thym)     #add half the size of Thymio\n",
    "    max_x, max_y = grid.shape[0], grid.shape[1]          # Size of the map\n",
    "    # List of all coordinates in the grid\n",
    "    x,y = np.mgrid[0:max_x:1, 0:max_y:1]\n",
    "    pos = np.empty(x.shape + (2,))\n",
    "    pos[:, :, 0] = x; pos[:, :, 1] = y\n",
    "    pos = np.reshape(pos, (x.shape[0]*x.shape[1], 2))\n",
    "    coords = list([(int(x[0]), int(x[1])) for x in pos])\n",
    "\n",
    "    # Define the heuristic, here = euclidian distance to goal ignoring obstacles\n",
    "    h = np.linalg.norm(pos - goal, axis=-1)\n",
    "    h = dict(zip(coords, h))\n",
    "\n",
    "    # Run the A* algorithm\n",
    "    path, closedSet = A_Star(start, goal, h, coords, occupancy_grid, visitedNodes)\n",
    "    path = np.array(path).reshape(-1, 2).transpose()\n",
    "\n",
    "    closedSet = np.array(closedSet).reshape(-1, 2).transpose()\n",
    "\n",
    "    convert_path(path, finalpath)                  #convert to a list of Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Initialization:**\n",
    "    The algorithm starts by checking if the start and goal points are within the boundaries of the map and whether they are in a traversable space. If not, an error message is shown and wait to move the point.\n",
    "    The possible movements are defined based on 8-connectivity, meaning the algorithm considers movements in eight possible directions (horizontal, vertical, and diagonal).\n",
    "    The A* algorithm uses sets (openSet and closedSet) and dictionaries (cameFrom, gScore, and fScore) to keep track of nodes and their associated scores.\n",
    "    \n",
    "    \n",
    "    For node n, cameFrom[n] is the node immediately preceding it on the cheapest path from start to n currently known.\n",
    "\n",
    "    For node n, gScore[n] is the cost of the cheapest path from start to n currently known.\n",
    "    \n",
    "    For node n, fScore[n] := gScore[n] + h(n). map with default value of Infinity\n",
    "\n",
    "#### Implementation <a class=\"anchor\" id=\"Aalgorithm\"></a>\n",
    "\n",
    "\n",
    "* **A-Algorithm Implementation:**\n",
    "    The A* algorithm is implemented using a while loop that continues until there are no more nodes to explore (openSet is empty) or the goal is reached.\n",
    "    In each iteration, the algorithm selects the node with the lowest fScore from the openSet. This node is set as the current node.\n",
    "    The neighbors of the current node are explored, and for each neighbor, the algorithm calculates tentative scores (gScore and fScore).\n",
    "    If a neighbor is not in the openSet, it is added. If the tentative gScore for a neighbor is lower than its current gScore, the algorithm updates the scores and backtracks the optimal path.\n",
    "    The algorithm continues to explore nodes until the goal is reached or all reachable nodes have been explored.\n",
    "\n",
    "* **Reconstruction of Path:**\n",
    "    If the goal is reached, the *reconstruct_path* function is called to reconstruct the optimal path from the start to the goal using the information stored in the cameFrom dictionary.\n",
    "    The reconstructed path is returned along with the set of nodes that were visited during the algorithm (closedSet).\n",
    "\n",
    "* **Visualization (Optional):**\n",
    "    The code includes optional visualization elements, such as displaying the visited nodes with a time delay 0f 0.05 seconds, to visualize the algorithm's progress.\n",
    "\n",
    "* **Error Handling:**\n",
    "    If the open set becomes empty and the goal has not been reached, a message is printed indicating that no path was found to the goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results <a class=\"anchor\" id=\"A*CCL*\"></a>\n",
    "\n",
    "We are happy with the results of our algorithm. It can very well compute the optimal path through the map. \n",
    "The main limit we can see is the time it takes to compute as it increases sharply when the grid is more precise: if the goal is far away, we need to wait a bit. Thus we needed to find an optimal size to reach a compromise between the precision of the position and the time of computation.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"image\\map_path_compute.jpg\" alt=\"pathcomputation\" title=\"Results path computation\" style=\"width:750px;\"/> \n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "     \"Results of the optimal path computation\"\n",
    "</div>\n",
    "\n",
    " \n",
    "The goal is labeled in red, and our thymio has a green arrow on it. The points in grey show the extended size of the obstacles taking in account the size of Thymio and the visited points in yellow when computing the path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "\n",
    "# Kalman Filtering <a class=\"anchor\" id=\"filtering\"></a>\n",
    "\n",
    "We decided to work with a Kalman filter because it is optimal for the robot localization due to its ability to handle noise and uncertainties effectively.\n",
    "\n",
    "Kalman filtering is used to estimate the state of a system based on a series of noisy and incomplete measurements. In the context of robotics and localization, Kalman filters are particularly beneficial for obtaining a good position estimation of a robot. The state of the robot is defined as its position, angle, angular velocity and velocity.\n",
    "\n",
    "We need to gather informations about the thymio's detected state:\n",
    "- overhead camera localization\n",
    "- odometry and thymio's target speed\n",
    "\n",
    "Furthermore, Kalman filters use a recursive estimation process, meaning that they continually update the state estimate as new measurements become available. This allows the filter to provide real-time position estimates, even if the camera is hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical model <a class=\"anchor\" id=\"mathmodel\"></a>\n",
    "\n",
    "The Thymio's movement is describe by its translation on the x and y axes and its rotation $\\theta$. \n",
    "It is ruled by the following motion laws :\n",
    "\n",
    "1. Forward speed :    $v = K*\\frac{dmot_R + dmot_L}{2}$\n",
    "2. Rotational speed : $w = K*\\frac{dmot_R - dmot_L}{\\pi * THYMIO\\_WIDTH}$\n",
    "3. Position :         $x = v \\cdot \\cos(\\theta) \\cdot dt$\n",
    "4. Position :         $y = v \\cdot \\sin(\\theta) \\cdot dt$\n",
    "5. Direction :        $\\theta = w \\cdot dt$\n",
    "\n",
    "(*_K being a conversion factor such that 1 thymio's speed equals K x 1 unit per second (our unit being a grid case))\n",
    "\n",
    "Our model is thus non linear due to theta !\n",
    " \n",
    "\n",
    "$$X = \n",
    "\\begin{bmatrix}\n",
    "v\\\\\n",
    "\\omega\\\\\n",
    "p_x\\\\\n",
    "p_y\\\\\n",
    "\\theta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where:\n",
    "- $v$ is the magnitude speed in $[pxl/s]$.\n",
    "- $\\omega$ is the angular speed $[rad/s]$.\n",
    "- $p_x$, $p_y$ is the cartesian position in $[pxl]$.\n",
    "- $\\theta$ is the orientation in $[rad]$.\n",
    "\n",
    "\n",
    "         | \n",
    "\n",
    "Our input is $U = [dmot_l, dmot_r]$, the delta between the new and the previous input motor speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on filtering <a class=\"anchor\" id=\"filterconcl\"></a>\n",
    "\n",
    "The extended Kalman filter satifies our needs for this project. It allows to merge the camera measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "## Motion Control <a class=\"anchor\" id=\"motioncontrol\"></a>\n",
    "\n",
    "To compute the speed of the Thymio, we decided to decompose the computation in three parts: orientation, position and obstacle detection.\n",
    "The final velocity will be composed of the three different speeds: $\\vec{v}_{total}=\\vec{v}_{orientation}+\\vec{v}_{forward}+\\vec{v}_{obstacle}= \\begin{bmatrix} v_L \\\\ v_R \\end{bmatrix}$\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"image/graph_motion.jpg\" alt=\"Motion Control\" title=\"A robot is moving around with the proposed motion framework\" style=\"width:300px;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "   A robot is moving around with the proposed motion framework \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orientation & angle error <a class=\"anchor\" id=\"velorientation\"> </a>\n",
    "\n",
    "We define orientation changes as an angle error defined by: $\\alpha _E= \\alpha _C - \\alpha _T$, where $\\alpha _T$ is the current angle of the thymio and $\\alpha _C$ is the desired angle change.\n",
    "\n",
    "With classic trigonometry formel we get that: $\\alpha _C = arctan(\\frac{P_{y+1}-P_{yT}}{P_{x+1}-P_{xT}})$, where $P_T$ the position of the Thymio and $P_{+1}$ the desired position of the Thymio.\n",
    "\n",
    "\n",
    "One of the big limitations found is because of the use of the angles functions. Indeed, it gives values between $[0; 2\\pi]$. So if in some cases we where to near of 0 or $2\\pi$, the robot just turns around infinitly. \n",
    "Therefore we decided to normalize all angles between $[-\\pi; \\pi]$, to prevent discontinuities and unexpected behavior as well as to take the shortest time to turn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_angle_error(position, checkpoint, thymio_direction):\n",
    "  \"\"\"\n",
    "  Computes the angle error between the robot and the goal.\n",
    "\n",
    "  :param position: Current position of the robot.\n",
    "  :param checkpoint: position to reach (next point in the global path).\n",
    "  :param thymio_angle: Current orientation of the robot.\n",
    "  :return: Angle error between the robot and the goal.\n",
    "  \"\"\"\n",
    "  goal_angle = np.arctan2( (checkpoint.y - position.y), (checkpoint.x - position.x))\n",
    "\n",
    "  thymio_direction = - thymio_direction    # Because the y-axis is inverted in the simulator  \n",
    "\n",
    "  angle_error = goal_angle - thymio_direction         \n",
    "  angle_error = (angle_error + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "  return angle_error\n",
    "\n",
    "\n",
    "\n",
    "def compute_derived_angle_error(angle_error, prev_angle_error, dt):\n",
    "  \"\"\"\n",
    "  Computes the derived angle error between the robot and the goal.\n",
    "\n",
    "  :param angle_error: Angle error between the robot and the goal.\n",
    "  :param prev_angle_error: Previous angle error between the robot and the goal.\n",
    "  :param dt: Time step.\n",
    "  :return: Derived angle error between the robot and the goal.\n",
    "  \"\"\"\n",
    "  return (angle_error - prev_angle_error) / dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PD Controler<a class=\"anchor\" id=\"controler\"> </a>\n",
    "\n",
    "We decided to use a PD controler because of it is simple to implement and it is suitable for fast, real-time feedback.\n",
    "\n",
    "First, we only had a $K_p$ controler, we could see that the Thymio made some oscillations in its motion, therefore we added the $K_d$ to counteract those undesired movement and so get a more stable response. The derivative helps to reduce the sensitivity of the noise.\n",
    "\n",
    "$K_{p}$ and $K_d$ are constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute velocity <a class=\"anchor\" id=\"veldirection\"> </a>\n",
    "\n",
    "\n",
    "* *'base_speed'* is the forward velocity that you want the robot to maintain.\n",
    "\n",
    "$v_{forward}=const.$\n",
    "\n",
    "The other calculated speeds are going to act as a penalty or gain on top of the *'base_speed'* for each wheel. \n",
    "\n",
    "* *'rot_speed'* is the rotational speed calculated using the PD controler based on the angle error (*'angle_error'*) and its derivative (*'derived_angle_error'*).\n",
    "\n",
    "$\\vec{v}_{orientation}=(K_{p} \\cdot \\alpha _E + K_{d} \\cdot \\frac{d \\alpha _E}{dt} )\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$.\n",
    "\n",
    "The [1,-1] matrix is because to turn, the two wheels need to have opposite speeds.\n",
    "\n",
    "\n",
    "$\\vec{v}_{wheel}= \\vec{v}_{forward} + \\vec{v}_{orientation}$\n",
    "\n",
    "\n",
    "The rotational speed is clipped to avoid saturation (>500). If the absolute value of rot_speed exceeds the limit 200 - base_speed, it is adjusted to this limit to prevent oversaturation and the program to crash.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controller(robot, checkpoint, base_speed, prev_angle_error, dt, final_goal_reached=False):\n",
    "  '''\n",
    "  This function computes the motor velocities based on the position, goal, and control parameters.\n",
    "  param robot : Current position of the robot\n",
    "  param checkpoint : Goal position to reach (next point in the global path)\n",
    "  param base_speed : Base speed of the robot\n",
    "  param final_goal_reached : Flag indicating whether the final goal is reached, not motion is needed.\n",
    "\n",
    "  return : Input as a tuple (MotL, MotR) \n",
    "  return : angle_error\n",
    "  '''\n",
    "  if final_goal_reached:\n",
    "      return 0, 0, 0\n",
    "  \n",
    "  angle_error = compute_angle_error(robot.position, checkpoint, robot.direction)\n",
    "  derived_angle_error = compute_derived_angle_error(angle_error, prev_angle_error, dt)\n",
    "\n",
    "  rot_speed = Kp * angle_error + Kd * derived_angle_error\n",
    "\n",
    "  # Clip the rotation speed to avoid saturation\n",
    "  rot_speed = rot_speed if abs(rot_speed) < 200 - base_speed else 200 - base_speed\n",
    "\n",
    "  MotL = base_speed + rot_speed\n",
    "  MotR = base_speed - rot_speed\n",
    "\n",
    "  return MotL, MotR, angle_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local navigation <a class=\"anchor\" id=\"localnavigation\"> </a>\n",
    "\n",
    "First we wanted to use the potential field Navigation system and totaly ignoring the other velocities when an obstacle is encountered. It came quickly out that the thymio easily gets out of the desired path. Therefore, we tried to do something similar, but the $\\vec{v}_{obstacle}$ will act as a gain on the total computed speed $\\vec{v}_{wheel}$. We used the same gains for the proximity sensors as in the exercise session 4 $^{[1]}$.\n",
    "\n",
    "The external sensors have a stronger weight than the internal ones. The Thymio will be 'repulsed' more quickly when it detects an obstacle, so it will no longer be in line with it.\n",
    "\n",
    "We also added a visualization function on the map to show the detected obstacles in real time. It was not very concluant during the video but wasn't a priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is similar as the one used in the merge version of the code, to make it work independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if needed decomment:\n",
    "\n",
    "#import tdmclient.notebook\n",
    "#await tdmclient.notebook.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_python\n",
    "timer_period[0] = 10  # 10ms sampling time\n",
    "\n",
    "@onevent \n",
    "def timer0():\n",
    "    global prox_horizontal, motor_left_target, motor_right_target\n",
    "    speed0 = 75     # nominal speed\n",
    "\n",
    "    # gains used with front proximity sensors [0,5]\n",
    "    weights = [6, 4, -3, -6, -8]\n",
    "\n",
    "    addLeft=0\n",
    "    addRight=0\n",
    "    diffDelta=0\n",
    "    # adjustment for obstacles, \"gradient\" due to obstacles\n",
    "    for i in range(5):\n",
    "        addLeft += prox_horizontal[i] * weights[i]//200\n",
    "        addRight += prox_horizontal[i] * weights[4 - i]//200\n",
    "    \n",
    "    motor_left_target = speed0 + addLeft\n",
    "    motor_right_target = speed0 + addRight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "<video width=\"640\" height=\"360\" controls autoplay>\n",
    "  <source src=\"image/local_avoidance.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results <a class=\"anchor\" id=\"MCresults\"> </a>\n",
    "The motion control works well when the parameters are tuned, the speed could be improved but this implementation assures the inputs given for the motor never reach the threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Conclusion <a class=\"anchor\" id=\"conclusion\"> </a>\n",
    "\n",
    "To conclude, we have achieved all the given tasks and are satisfied with our results. We tried differents approaches to solve each part which took some time to reach concluant results, especially for the vision part before using Arucos as the light conditions influences the efficiency. We saw some limits of our code: we implemented a projection function at the begining to avoid distortions on the map but it brought some problems in the detection of Aruco afterward, we have therefore done without as it doesn't bring much error. The choice of a grid for the mapping also brought some computational problem for bigger maps. Using the coordinate system of the image was a challenge as it is non conventional for motion and needed conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources :\n",
    "\n",
    "\n",
    "[1] MICRO-452 \"Basics of mobile robotics\" course of EPFL from Professor Francesco Mondada\n",
    "\n",
    "[2] « Kinematics — CoopRobo 1.0.0 documentation » https://cooprobo.readthedocs.io/en/latest/mobile/pioneer/model/kinematics.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
