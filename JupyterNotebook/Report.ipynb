{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MICRO-452 Basics of mobile robotics\n",
    "\n",
    "## Project Report\n",
    "\n",
    "Team  13:\n",
    "\n",
    "Wilhelm Laetitia,\n",
    "Beuret Sylvain,\n",
    "Labbe Victor,\n",
    "Jaffal Oussama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Computer vision](#computervision)\n",
    "    * [Set up](#setup)\n",
    "    * [Libraries](#libraries)\n",
    "    * [Map, Thymio, and obstacles detection](#detection)\n",
    "    * [Visibility graph](#visgraph)\n",
    "    * [Results](#results)\n",
    "  \n",
    "* [Path planning](#pathplanning)\n",
    "    * [STEP 1](#step1)\n",
    "  \n",
    "* [Filtering](#filtering)\n",
    "    * [General, filtering](#genfiltering)\n",
    "    * [Mathematical model](#mathmodel)\n",
    "    * [Filter blabla](#filterblabla)\n",
    "    * [Conclusion on filtering](#filterconcl)\n",
    "    \n",
    "* [Navigation](#navigation)\n",
    "    * [Global navigation](#globalnavigation)\n",
    "\n",
    "* [Motion control](#motioncontrol)\n",
    "    * [Orientation velocity](#velorientation)\n",
    "    * [Direction velocity](#veldirection)\n",
    "    * [Local navigation](#localnavigation)\n",
    "    \n",
    "* [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "\n",
    "This project is made in the frame of the EPFL course: Basics of Mobile Robotics (BoMR). The goal is to program a robot, the Thymio, to go from a specific point to another while avoiding the obstacles and surviving some kidnapping sessions. In order to achieve this, we used computer vision, filtering, global navigation and local navigation.\n",
    "\n",
    "As you can see, we are on a car racing circuit, where our beloved thymio is racing against the clock.\n",
    "\n",
    "Our program uses an overhead camera to detect the obstacles, the track, the thymio and the finishing line. The global navigation part, anaibles to do path planning to reach the line (goal). Finally we have a Kalman filter, using the robot's poistion from the camera and odometry, allowing the thymio to know and correct its position in real time, even with a hidden camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AJOUTER NOTRE VIDEO\n",
    "\n",
    "from IPython.display import Video\n",
    "\n",
    "video_path = 'chemin/vers/votre/video.mp4'\n",
    "video = Video(video_path)\n",
    "\n",
    "#affiche la vidéo\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the camera is on, different options can be displayed, by pressing the following keys: \n",
    "* p: take a picture\n",
    "* v: start recording a video\n",
    "* w: stop recording\n",
    "* esp: Quits the program.\n",
    "\n",
    "The camera requires around one minute to launch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global set-up <a class=\"anchor\" id=\"computervision\"></a>\n",
    "\n",
    "### Map Set-up <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "The road is in white and the obstacles are in black. We have in addition some white 3D obstacles for local avoidance, not on the picture.\n",
    "\n",
    "\n",
    "<img src=\"image/setup_map.jpg\" alt=\"Setup Map\" title=\"Map\" style=\"width:400px;\"/> \n",
    "\n",
    "### Libraries & Constants<a class=\"anchor\" id=\"libraries\"></a>\n",
    "\n",
    "Using Numpy, OpenCv and Shapely libraries, we get a livestream from the webcam and we use the 20th frame of the video to detect most of the things that we need to plan the shortest path for the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from tdmclient import ClientAsync, aw\n",
    "import time\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "import threading\n",
    "import keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "GRID_RES = 25\n",
    "TIMESTEP = 0.1\n",
    "SIZE_THYM = 2.0  #size of thymio in number of grid\n",
    "LOST_TRESH = 10 #treshold to be considered lost\n",
    "REACH_TRESH = 3 #treshhold to reach current checkpoint\n",
    "GLOBAL_PLANNING = True\n",
    "GOAL_REACHED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=\"red\">### EXPLAIN MEANING OF THE CONSTANTS###</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented several classes, so we can use the same variables along the entire code coming from different persons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch demonstration <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "To run our project, you need to download this [GitHub repository](https://github.com/Thebestvic07/BOMR_Project.git).\n",
    "\n",
    "To launch our demo, you need to follow those steps:\n",
    "1. Run the main.py\n",
    "2. Don't put anything on the map except of the obstacles\n",
    "3. When \"searching thymio and goal\" is written in the terminal, you can add both\n",
    "4. The global path will show the path to follow\n",
    "4. Press \"p\" to exist the demo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<font color=\"red\">### EXPLAIN HOW THE MAIN WITH THE THREADS IS WORKING ###</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main.py structure <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "We decided to use the different following thread in the main so several task can run in parallel.\n",
    "\n",
    "\n",
    "##### 1. Camera Thread (<span style=\"text-decoration: underline;\">*camera_thread*</span>):\n",
    "\n",
    "**Function**: run_camera\n",
    "\n",
    "**Purpose**: Captures video frames from the camera, detects ArUco markers, updates the robot and goal positions based on camera data.\n",
    "\n",
    "**Execution**: The run_camera function is executed in this thread. It continuously captures frames, processes ArUco markers, and updates the robot and goal positions.\n",
    "\n",
    "##### 2. Thymio Thread (<span style=\"text-decoration: underline;\">*thymio_thread*</span>):\n",
    "    \n",
    "**Function**: update_thymio\n",
    "\n",
    "**Purpose**: Updates the Thymio's variables, such as sensor data, every 0.1 seconds\n",
    "\n",
    "**Execution**: The update_thymio function is executed in this thread. It continuously reads variables from the Thymio robot, updating its state.\n",
    "\n",
    "##### 3. Display Thread (<span style=\"text-decoration: underline;\">*display_thread*</span>):\n",
    "\n",
    "**Function**: display\n",
    "\n",
    "**Purpose**: Displays the current state of the robot and goal on the screen.\n",
    "\n",
    "**Execution**: The display function is executed in this thread. It continuously updates the display based on the robot's position, goal, and the environment's state.\n",
    "\n",
    "##### 4. Main Thread:\n",
    "\n",
    "**Function**: __main__\n",
    "\n",
    "**Purpose**: The main thread that orchestrates the overall control flow.\n",
    "**Execution: The script starts by initializing various objects, threads, and constants. It then enters the main loop, which handles tasks such as Thymio initialization, path planning, motion updates, obstacle avoidance, and goal-reaching conditions. The main loop is controlled by a timer, and the script continuously monitors the state of the robot and the environment.\n",
    "\n",
    "\n",
    "The **Daemon** parameter is always set to True, so when the threads closes when the main program exits.\n",
    "\n",
    "Thread Interactions:\n",
    "\n",
    "The camera thread continuously updates the robot and goal positions based on camera data.\n",
    "The Thymio thread continuously reads variables from the Thymio robot.\n",
    "The display thread continuously updates the display based on the robot's position, goal, and the environment's state.\n",
    "The main thread orchestrates the overall control flow, handles path planning, motion control, obstacle avoidance, and goal-reaching conditions.\n",
    "\n",
    "Note:\n",
    "\n",
    "The daemon=True parameter for threads means that these threads will be terminated when the main program exits. If they were non-daemon threads, the program would wait for these threads to finish before exiting.\n",
    "The threading module is used for parallel execution of these tasks, allowing the script to perform multiple actions concurrently.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision <a class=\"anchor\" id=\"computervision\"></a>\n",
    "\n",
    "\n",
    "### Map, Thymio and obstacles detection<a class=\"anchor\" id=\"detection\"></a>\n",
    "\n",
    "After trying a lot of days to do shape detections, we decided to use the aruco technic to detect the thymio, the goal.\n",
    "\n",
    "The obstacles are in black, so the thymio can go everywhere it is white.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map detection<a class=\"anchor\" id=\"mapdetection\"></a>\n",
    "\n",
    "The map detection is only done once at the begining of the timeline. We first take a single frame, with only the black ostacles and one aruco. The aruco is there so we can compute the grid resolution. It is a calibration image.\n",
    "\n",
    "The creation of the black and white image involves the is_black_cell function, which evaluates whether a grid cell in the captured image is predominantly black. This function takes an image as input and determines whether the cell contains more than 1/8 black pixels. The process includes converting the image to grayscale, creating a binary mask using a specified threshold, calculating the percentage of black pixels, and comparing it to a threshold percentage (here, set to 10%). If the percentage of black pixels surpasses the threshold, the cell is categorized as black, signifying the presence of obstacles, otherwise, it is classified as white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_black_cell(image):\n",
    "    # Know if the grid cell contains more than 1/8 pixels who are black\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_mask = cv2.threshold(image, 110, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Calculate the percentage of black pixels in the image\n",
    "    total_pixels = image.size\n",
    "    black_pixels = total_pixels - cv2.countNonZero(binary_mask)\n",
    "    percentage_black = (black_pixels / total_pixels) * 100\n",
    "\n",
    "    # Set the threshold percentage\n",
    "    threshold_percentage = 10  # Adjust this threshold as needed\n",
    "\n",
    "    # Check if the percentage of black pixels exceeds the threshold\n",
    "    if percentage_black > threshold_percentage:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to generate a simplified black and white representation of the environment, highlighting obstacle positions for further processing in Thymio navigation and obstacle avoidance.\n",
    "\n",
    "The map is created when the following line is called in the main. It will store the builtmap as an image \"capture_frame.jpq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builtmap, grid_res = apply_grid_to_camera(DEFAULT_GRID_RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image\\captured_frame.png\" alt=\"Setup Map\" title=\"Map\" style=\"width:400px;\"/>  \"captured_frame.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aruco detection<a class=\"anchor\" id=\"arucodetection\"></a>\n",
    "\n",
    "The Aruco markers are detected using the Aruco marker detection functionality provided by the OpenCV library. The cv2.aruco.getPredefinedDictionary function is used to obtain a predefined Aruco marker dictionary, and cv2.aruco.DetectorParameters is employed to configure the marker detection parameters. The cv2.aruco.detectMarkers function is then applied to find and extract Aruco markers' corners and IDs in the camera frame.\n",
    "\n",
    "We will use those informations to get the positions of the Thymio and the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_arucos(corners, ids, image):\n",
    "\n",
    "    arucos = []\n",
    "    if len(corners) > 0:\n",
    " \n",
    "        ids = ids.flatten()\n",
    " \n",
    "        for (markerCorner, markerID) in zip(corners, ids):\n",
    "            corners = markerCorner.reshape((4, 2))\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = corners\n",
    " \n",
    "            topRight = (int(topRight[0]), int(topRight[1]))\n",
    "            bottomRight = (int(bottomRight[0]), int(bottomRight[1]))\n",
    "            bottomLeft = (int(bottomLeft[0]), int(bottomLeft[1]))\n",
    "            topLeft = (int(topLeft[0]), int(topLeft[1]))\n",
    " \n",
    "            cX = int((topLeft[0] + bottomRight[0]) / 2.0)\n",
    "            cY = int((topLeft[1] + bottomRight[1]) / 2.0)\n",
    "            \n",
    "            arucos.append((cX, cY, markerID, (bottomLeft, topLeft)))\n",
    "            \n",
    " \n",
    "    return image, arucos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thymio detection<a class=\"anchor\" id=\"thymiodetection\"></a>\n",
    "\n",
    "The ArUco marker 95 is placed on the Thymio. \n",
    "\n",
    "The ArUco identification gives us relevant information such as the center coordinates and orientation of the Thymio.\n",
    "\n",
    "If the Thymio is not detected (hidden camera for example), it will return a (0,0) position. So the kalman function can continue working without knowing the current localization of the Thymio.\n",
    "\n",
    "The choice to return this position is due that our reference ArUcO is based at this localisation. So when we are doing the obstacle map, it will be considered such as being an obstacle. The Thymio cannot be at (0,0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal detection <a class=\"anchor\" id=\"goaldetection\"></a>\n",
    "\n",
    "The goal detection process relies on another ArUco marker (n°99) specifically placed to represent the goal location. Similar to Thymio detection, the ArUco marker for the goal is identified in the captured frame. The center coordinates of this marker are extracted to determine the goal's position in the grid.\n",
    "\n",
    "The variable \"last_known_goal_pos\" is updated only when the measure goal_position is diffenrent of (0,0). We consider that the goal will not change of position if the camera is hidden. Therefore we are working with the \"last_known_goal_pos\" variable in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obstacles detection <a class=\"anchor\" id=\"obstaclesdetection\"></a>\n",
    "\n",
    "As explained above, the obstacle detection is based on the assumption that non-white cells in the environment are representing obstacles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results <a class=\"anchor\" id=\"results\"></a>\n",
    "\n",
    "The results are in the majority of the time good. Again we mostly have problem with the time it takes the camera to connect to the computer. Therefore we put waiting nodes, to let everything get in place before launching the \"race\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the vision, you can put the following code in the camera.py file and run it. It will print the detected Thymio and goal position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# import math \n",
    "# import time\n",
    "# import cv2.aruco as aruco\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "\n",
    "#     last_known_goal_pos = (0, 0)\n",
    "#     robot_pos = (0, 0)\n",
    "\n",
    "#     arucos = get_arucos(cap.read()[1])[1]\n",
    "\n",
    "#     grid_resolution = get_dist_grid(arucos)\n",
    "\n",
    "#     width, height = 707, 10007\n",
    "#     # width, height = set_param_frame(arucos)\n",
    "\n",
    "#     while cap.isOpened():\n",
    "#         frame = cap.read()[1]\n",
    "\n",
    "#         frame, arucos = get_arucos(frame)\n",
    "\n",
    "#         frame, arucos, robot_pos, angle = show_robot(frame, arucos, grid_resolution)\n",
    "#         goal_pos = center_in_grid(arucos, 99, grid_resolution)\n",
    "\n",
    "#         if goal_pos != (0, 0):\n",
    "#             last_known_goal_pos = goal_pos\n",
    "\n",
    "#         draw_goal(frame, arucos, grid_resolution)\n",
    "\n",
    "#         if check_if_goal_reached(arucos, robot_pos, last_known_goal_pos):\n",
    "#             print('Goal reached')\n",
    "#             break\n",
    "\n",
    "#         # frame = projected_image(frame, arucos, width, height)\n",
    "#         cv2.imshow(\"Video Stream\", frame)\n",
    "\n",
    "#         print(\"Robot position: \", robot_pos)\n",
    "#         print(\"Goal position: \", last_known_goal_pos)\n",
    "        \n",
    "#         key = cv2.waitKey(1) & 0xFF\n",
    "#         if key == ord(\"q\"):\n",
    "#             break\n",
    "\n",
    "#     cv2.destroyAllWindows()\n",
    "#     cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path planning<a class=\"anchor\" id=\"pathplanning\"></a>\n",
    "\n",
    "The goal of the path planning is to find the fastest, so the shortest, path for Thymio to reach the final line. It takes as input the visibility graph of the map, and returns the optimal path, which is a list of coordinates for Santo to follow. \n",
    "\n",
    "Where do we get our informations ?\n",
    "\n",
    "Thanks to the previous steps, we get a global map composed of white and black cells (\"capture_frame.jpg\") that enables us to know where the thymio can go through.\n",
    "\n",
    "The start and end positions are defined by the initial position of the Thymio (aruco 95) and the goal (aruco 99) last detected position \"last_known_goal_pos\".\n",
    "\n",
    "The Path Planning algorithm used was inspired by the A* algorithm seen in the course excercise session [1].\n",
    "\n",
    "Path Following Module: the estimation of the robot's position is given by the kalman_filter function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global navigation <a class=\"anchor\" id=\"globalnavigation\"></a>\n",
    "\n",
    "We decided to us the A* algorithm for the path planning. The goal is to find the optimal path from a starting point to a goal point, considering obstacles in the environment.\n",
    "\n",
    "Prior computing the optimal path, we had a half thymio length to all obstacle sides, so that the Thymio, considered as a point, want run in it. Now we can calculate the path using the A* algorithm,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A* algorithm <a class=\"anchor\" id=\"Aalgorithm\"></a>\n",
    "\n",
    "\n",
    "We chose to use the A* algorithm because it optimizes the search of the optimal path by expanding the most promising node chosen according some cost and heuristic functions.\n",
    "\n",
    "<img src=\"image\\A-algorithm-explanation.png\" alt=\"A algo slide\" title=\"A * algorithm\" style=\"width:400px;\"/>  \"A* algorithm, extending Dijkstra\" [1]\n",
    "\n",
    "Functions:\n",
    "\n",
    "$f(n)=g(n)+h(n)$\n",
    "\n",
    "$f(n)$: The f-score of node nn\n",
    "\n",
    "$g(n)$: The cost of the cheapest path from the start node to node nn\n",
    "\n",
    "$h(n)$: The heuristic estimate of the cost from node nn to the goal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Initialization:**\n",
    "    The algorithm starts by checking if the start and goal points are within the boundaries of the map and whether they are in a traversable space. If not, an exception is raised.\n",
    "    The possible movements are defined based on 8-connectivity, meaning the algorithm considers movements in eight possible directions (horizontal, vertical, and diagonal).\n",
    "    The A* algorithm uses sets (openSet and closedSet) and dictionaries (cameFrom, gScore, and fScore) to keep track of nodes and their associated scores.\n",
    "    \n",
    "    \n",
    "    For node n, cameFrom[n] is the node immediately preceding it on the cheapest path from start to n currently known.\n",
    "\n",
    "    For node n, gScore[n] is the cost of the cheapest path from start to n currently known.\n",
    "    \n",
    "    For node n, fScore[n] := gScore[n] + h(n). map with default value of Infinity\n",
    "\n",
    "#### Implementation <a class=\"anchor\" id=\"Aalgorithm\"></a>\n",
    "\n",
    "\n",
    "* **A-Algorithm Implementation:**\n",
    "    The A* algorithm is implemented using a while loop that continues until there are no more nodes to explore (openSet is empty) or the goal is reached.\n",
    "    In each iteration, the algorithm selects the node with the lowest fScore from the openSet. This node is set as the current node.\n",
    "    The neighbors of the current node are explored, and for each neighbor, the algorithm calculates tentative scores (gScore and fScore).\n",
    "    If a neighbor is not in the openSet, it is added. If the tentative gScore for a neighbor is lower than its current gScore, the algorithm updates the scores and backtracks the optimal path.\n",
    "    The algorithm continues to explore nodes until the goal is reached or all reachable nodes have been explored.\n",
    "\n",
    "* **Reconstruction of Path:**\n",
    "    If the goal is reached, the *reconstruct_path* function is called to reconstruct the optimal path from the start to the goal using the information stored in the cameFrom dictionary.\n",
    "    The reconstructed path is returned along with the set of nodes that were visited during the algorithm (closedSet).\n",
    "\n",
    "* **Visualization (Optional):**\n",
    "    The code includes optional visualization elements, such as displaying the visited nodes with a time delay 0f 0.05 seconds, to visualize the algorithm's progress.\n",
    "\n",
    "* **Error Handling:**\n",
    "    If the open set becomes empty and the goal has not been reached, a message is printed indicating that no path was found to the goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion <a class=\"anchor\" id=\"A*CCL*\"></a>\n",
    "\n",
    "We are happy with the results of our algorithm. It can very well compute the optimal path through the map. \n",
    "The main limit we can see is the time it takes to compute it: if the goal is far away, we need to wait a bit.\n",
    "\n",
    "<img src=\"image\\map_path_compute.jpg\" alt=\"pathcomputation\" title=\"Results path computation\" style=\"width:750px;\"/>  \"Results of the optimal path computation\" [1]\n",
    "\n",
    "The goal is labeled in red, and our thymio has a green arrow on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalmann Filtering <a class=\"anchor\" id=\"filtering\"></a>\n",
    "\n",
    "We decided to work with a Kalman filter because it is optimal for the robot localization due to its ability to handle noise and uncertainties effectively.\n",
    "\n",
    "Kalman filtering is used to estimate the state of a system based on a series of noisy and incomplete measurements. In the context of robotics and localization, Kalman filters are particularly beneficial for obtaining a good position estimation of a robot. The state of the robot is defined as its position, angle, angular velocity and velocity.\n",
    "\n",
    "We need to gather informations about the thymio's detected state:\n",
    "- overhead camera localization\n",
    "- odometry and thymio's target speed\n",
    "\n",
    "Furthermore, Kalman filters use a recursive estimation process, meaning that they continually update the state estimate as new measurements become available. This allows the filter to provide real-time position estimates, even if the camera is hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical model <a class=\"anchor\" id=\"mathmodel\"></a>\n",
    "\n",
    "The Thymio's movement is describe by its translation on the x and y axes and its rotation $\\theta$. \n",
    "It is ruled by the following motion laws :\n",
    "\n",
    "1. Forward speed :    $v = K*\\frac{dmot_R + dmot_L}{2}$\n",
    "2. Rotational speed : $w = K*\\frac{dmot_R - dmot_L}{\\pi * THYMIO\\_WIDTH}$\n",
    "3. Position :         $x = v \\cdot \\cos(\\theta) \\cdot dt$\n",
    "4. Position :         $y = v \\cdot \\sin(\\theta) \\cdot dt$\n",
    "5. Direction :        $\\theta = w \\cdot dt$\n",
    "\n",
    "(*_K being a conversion factor such that 1 thymio's speed equals K x 1 unit per second (our unit being a grid case))\n",
    "\n",
    "Our model is thus non linear due to theta !\n",
    " \n",
    "\n",
    "$$X = \n",
    "\\begin{bmatrix}\n",
    "v\\\\\n",
    "\\omega\\\\\n",
    "p_x\\\\\n",
    "p_y\\\\\n",
    "\\theta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where:\n",
    "- $v$ is the magnitude speed in $[pxl/s]$.\n",
    "- $\\omega$ is the angular speed $[rad/s]$.\n",
    "- $p_x$, $p_y$ is the cartesian position in $[pxl]$.\n",
    "- $\\theta$ is the orientation in $[rad]$.\n",
    "\n",
    "\n",
    "         | \n",
    "\n",
    "Our input is $U = [dmot_l, dmot_r]$, the delta between the new and the previous input motor speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on filtering <a class=\"anchor\" id=\"filterconcl\"></a>\n",
    "\n",
    "The extended Kalman filter satifies our needs for this project. It allows to merge the camera measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"inserer code\")\n",
    "\n",
    "heuristic normal euclidienne pour prendre en compte les diagonales, puis faire A*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Control <a class=\"anchor\" id=\"motioncontrol\"></a>\n",
    "\n",
    "To compute the speed of the Thymio, we decided to decompose the computation in 3 parts: orientation, position and obstacle detection.\n",
    "The final velocity will be composed of the three different speeds: $\\vec{v}_{total}=\\vec{v}_{orientation}+\\vec{v}_{direction}+\\vec{v}_{obstacle}= \\begin{bmatrix} v_L \\\\ v_R \\end{bmatrix}$\n",
    "\n",
    "\n",
    "<img src=\"image/motion_control.png\" alt=\"Motion Control\" title=\"A robot is moving around with the proposed motion framework\" style=\"width:300px;\"/>  \"A robot is moving around with the proposed motion framework\" [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orientation velocity <a class=\"anchor\" id=\"velorientation\"> </a>\n",
    "\n",
    "We define orientation changes as an angle error defined by: $\\alpha _E= \\alpha _R - \\alpha _C$, where $\\alpha _R$ is the current angle of the thymio and $\\alpha _C$ is the desired angle change.\n",
    "\n",
    "With classic trigonometry formel we get: that $\\alpha _C = arctan(\\frac{P_{y+1}-P_{yR}}{P_{x+1}-P_{xR}})$, where $P_R$ the position of the robot and $P_{+1}$ the desired position of the Thymio.\n",
    "\n",
    "\n",
    "$\\alpha _E$ is the feedback control with a P controler. The [1,-1] matrix is because to turn, the two wheels need to have opposite speeds. $K_{P \\alpha}$ scales the error.\n",
    "\n",
    "So:  $\\vec{v}_{orientation}=K_{P \\alpha} * \\alpha _E \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "One of the big limitations found is because of the use of the arctan function. Indeed, it only gives values between $[0; 2\\pi]$. So if in some cases we where to near of 0 or $2\\pi$, the robot just turns around infinitly. So we decided to get a value only between $[-\\pi; \\pi]$ with a modulo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction velocity <a class=\"anchor\" id=\"veldirection\"> </a>\n",
    "\n",
    "Now that the thymio is well axed, it only need to go straigh forward. To constrain the forward movement, we add a second scaling value: $180 - |\\alpha _E|$. So if the change in orientation is too big, it will furst turn then go forward. We will have again a $K_{PP}$. The second scaling factor avoids to make another if loop, where if $\\alpha _E$ is too big, it should first turn then move forward.\n",
    "\n",
    "So:  $\\vec{v}_{direction}=K_{PP} * (180 - |\\alpha _E|) \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controler <a class=\"anchor\" id=\"controler\"> </a>\n",
    "\n",
    "$K_{PP}$ and $K_{\\alpha}$ get different values depending on their position between two point of the global path. At the beginning, the $K_{\\alpha}$ will be greater than $K_{PP}$ so it is in the good axe, then the thymio will move forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local navigation <a class=\"anchor\" id=\"localnavigation\"> </a>\n",
    "\n",
    "First we wanted to use the potential field Navigation system totaly ignoring the other factors, like in the excercise session, but it came out that the thymio to easily get out of the desired path. Therefore, we tried to do something but with different weight on the different sensors.\n",
    "\n",
    "The internal sensors have a stronger weight than the external one, because the thymio will need to do a bigger move to avoid it. In the opposite, the external ones have weaker weights, so that if a thymio detects an obstacle, it will move less strongly, so it will get less out of the desired path. And inderectly it solved another problem, the thymio get less stuck with bigger obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is similar as the one used in the merge version of the code, to make it work independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if needed decomment:\n",
    "\n",
    "#import tdmclient.notebook\n",
    "#await tdmclient.notebook.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_python\n",
    "timer_period[0] = 10  # 10ms sampling time\n",
    "\n",
    "@onevent \n",
    "def timer0():\n",
    "    global prox_horizontal, motor_left_target, motor_right_target\n",
    "    # acquisition from ground sensor\n",
    "    speed0 = 75     # nominal speed\n",
    "\n",
    "    # gains used with front proximity sensors [0,5]\n",
    "    weights = [6, 4, -3, -6, -8]\n",
    "\n",
    "    addLeft=0\n",
    "    addRight=0\n",
    "    diffDelta=0\n",
    "    # adjustment for obstacles (\"gradient\" due to obstacles)\n",
    "    for i in range(5):\n",
    "        addLeft += prox_horizontal[i] * weights[i]//200\n",
    "        addRight += prox_horizontal[i] * weights[4 - i]//200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a class=\"anchor\" id=\"conclusion\">\n",
    "\n",
    "To conclude, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources :\n",
    "\n",
    "\n",
    "[1] MICRO-452 \"Basics of mobile robotics\" course of EPFL from Professor Francesco Mondada\n",
    "\n",
    "[2] « Kinematics — CoopRobo 1.0.0 documentation » https://cooprobo.readthedocs.io/en/latest/mobile/pioneer/model/kinematics.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
